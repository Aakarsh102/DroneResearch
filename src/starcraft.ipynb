{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "# from huggingface_hub import snapshot_download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aakarshrai/VSprojects/VFL/src\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['track']\n",
      "Track shape: (57, 206, 18)\n",
      "[ 56  16  94  93 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  38  79 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  44  77 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  69  19 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  62 144 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 128  19 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  96  82 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  18 160 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  36  81 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  69 153 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  62 147 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  67  21 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  26 163 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  60 132 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  18 204 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 161 153 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 217 162 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 211 164 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 208 164 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 177 223 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 134 212 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 228  19 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 224  19 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 238  99 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 195  99 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 130 221 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 136 210 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 229  78 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 172 224 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16  61 111 127   1 185 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 71  16  18  36 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16  86 171 127   1 108 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  20  23 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  31  19 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 235 218 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 24  16  85 139 127   1 226 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 24  16 170 101 127   1  99 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 224 221 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  18  30 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[143   1  36  29 169   1 208 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[125   1  29  26 169   1  83 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  26  29 169   1 174 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  26  31 169   1  88 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  26  35 169   1  86 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[153   1  36  36 249   3 221 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 28   2 228 215 169   1 221 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 222 215 169   1 207 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 229 206 169   1 201 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 229 210 169   1  58 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 229 211 169   1 239 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 229 213 169   1  10 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[125   1  33  26 169   1  45 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  27  26 169   1  52 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  26  26 169   1 130 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[143   1  36  33 169   1 215 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[143   1  36  31 169   1 223 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[131   1  31  31 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 71  16 237 204 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16  97 122 127   1 186 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  24  19 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 226 223 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16 169  70 127   1 236 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  29  18 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 190 208 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  20  28 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 231 103 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 74  16 187 200 127   1 104 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 235  23 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 186  88 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 179 225 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 220 153 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 123 221 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 127 221 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 193 102 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 188 216 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 238  86 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 235  79 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 161 147 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 152 160 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  76 152 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 109  80 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  24 137 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 74  16  68  41 127   1 232 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  24 221 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  60 142 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  15 147 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  15 153 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  31 221 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  78  18 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  17 155 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  47  77 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  29 223 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  36 222 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  94  88 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  36  86 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  18 140 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  17 149 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  20 162 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  74  18 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  71 151 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 121  28 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 101  79 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  94  84 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  92  90 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  35  88 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  76  16 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 134 218 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 130  18 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  20 218 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  18 210 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 137  18 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  83  17 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 219 155 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 237 100 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 238  91 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 181 223 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 184  90 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 132 219 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 159 158 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 161 156 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 163 151 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 215 164 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 240  88 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 193  93 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 182 223 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 190 214 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 231  19 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 237  30 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 202 164 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 163 141 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 118 222 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16 176  75 127   1 187 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  22  21 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16  93 115 127   1 241 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16 194 129 127   1  57 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 119  36 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[  6  16 159 186 129   1 127 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[125   1  26  27 169   1 161 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  36  26 169   1  72 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[ 28   2 229 215 169   1 176 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 224 215 169   1 207 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 229 208 169   1 245 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 220 215 169   1 239 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 219 215 169   1 129 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 28   2 226 215 169   1 210 255   5   0 255  14   3 233 219   0 160   0]\n",
      "[ 22   2 224 210 169   1 159 255 255   0 255   0   0   0   0   0   0   0]\n",
      "[125   1  35  26 169   1  15 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  26  33 169   1 175 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[125   1  31  26 169   1  79 255   0   0 255  14   1  22  21   0 132   0]\n",
      "[ 71  16  36  18 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 136 204 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 231 221 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  27  19 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  65  33 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 235 212 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 220 146 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 237 210 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16 158 118 127   1  59 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16 162 126 127   1 114 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  35  94 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 233 219 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 228 221 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 71  16 219 222 169   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 43  16  79 165 127   1  59 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[  6  16  96  54 129   1 254 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 237  36 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 195 109 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 179  89 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 146 161 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 125 223 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 121  23 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 188 219 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 191  91 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 219 160 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 240  93 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 237  81 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 210 165 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 154 162 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 235  28 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 233  21 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 188  90 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 193  97 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 186 221 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 155 160 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16 219  18 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 226  18 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  92 100 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 78  16  53  76 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 119  30 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16 125  19 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  65  27 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  73  18 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  63 149 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  17 142 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  40  77 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 58  16  45  75 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  20 212 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  22 219 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  27 221 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 132  19 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 123  21 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  62 138 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  67 151 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16  67  25 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 100  81 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n",
      "[ 56  16 103  81 127   1 191 255   0   0 255   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "tracks_dir = \"../StarCraft-Motion/tracks\"\n",
    "for fname in os.listdir(tracks_dir):\n",
    "    if not fname.endswith(\".npz\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(tracks_dir, fname)\n",
    "    # load the archive\n",
    "    shard = np.load(path, allow_pickle=True)\n",
    "\n",
    "    # see what keys are inside\n",
    "    print(shard.files)  \n",
    "    # e.g. [\"track\", \"metadata\", \"video\"]  (might vary slightly)\n",
    "\n",
    "    # pull them out\n",
    "    track    = shard[\"track\"]        # → np.ndarray, shape (T,N,18), dtype=uint8\n",
    "    metadata = shard.keys()  # → Python dict\n",
    "    # print(metadata)\n",
    "    # print(track[0])\n",
    "    # if there’s also a 'video' key, it’ll be raw bytes you can write out:\n",
    "    # with open(\"tmp.mp4\",\"wb\") as f: f.write(shard[\"video\"].tobytes())\n",
    "\n",
    "    # now exactly like before:\n",
    "    # print(fname, \"→\", track.shape, \"frames;\", metadata[\"game\"][\"map_name\"])\n",
    "    break\n",
    "print(\"Track shape:\", track.shape)\n",
    "# print(\"track n:\" track[0].shape[0])\n",
    "for i in track[0]:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ----- Paths to StarCraft-Motion data -----\n",
    "TRACKS_DIR = \"../StarCraft-Motion/tracks\"\n",
    "# pick one shard or loop through many; for demo we use the first\n",
    "shard_file = next(f for f in os.listdir(TRACKS_DIR) if f.endswith(\".npz\"))\n",
    "npz_path = os.path.join(TRACKS_DIR, shard_file)\n",
    "\n",
    "# ----- Load track tensor -----\n",
    "shard = np.load(npz_path, allow_pickle=True)\n",
    "track = shard[\"track\"]            # shape (T, N, 18), uint8\n",
    "\n",
    "# ----- Build dataset: flatten (T, N) into samples -----\n",
    "samples = track.reshape(-1, 18).astype(np.float32)\n",
    "# filter out destroyed units (status=5 at idx=5), if desired:\n",
    "# samples = samples[samples[:,5] != 5]\n",
    "\n",
    "\n",
    "# inputs and labels\n",
    "y = torch.from_numpy(samples[:, 11].astype(np.int64))  # coarse goal type labels\n",
    "X = torch.from_numpy(samples)\n",
    "f_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # feature indices to keep\n",
    "client_splits = [X[:, 0]]\n",
    "client_splits.append([X[:, 1]])\n",
    "client_splits.append([X[:, i] for i in range(2,5)])\n",
    "for i in range(5, 11):\n",
    "    client_splits.append(X[:, i])\n",
    "\n",
    "# ----- Split features across two clients -----\n",
    "# Client 1 sees state features [0:9), Client 2 sees [9:18)\n",
    "feature_dims = [9, 9]\n",
    "client_splits = torch.split(X, feature_dims, dim=1)\n",
    "dataset = TensorDataset(*client_splits, y)\n",
    "loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.7.1-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 setuptools-80.9.0 sympy-1.14.0 torch-2.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aakarshrai/VSprojects/VFL/vfl/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ----- Paths to StarCraft-Motion data -----\n",
    "TRACKS_DIR = \"../StarCraft-Motion/tracks\"\n",
    "shard_file = next(f for f in os.listdir(TRACKS_DIR) if f.endswith(\".npz\"))\n",
    "npz_path = os.path.join(TRACKS_DIR, shard_file)\n",
    "shard = np.load(npz_path, allow_pickle=True)\n",
    "track = shard[\"track\"]            # (T, N, 18), uint8\n",
    "\n",
    "# ----- j -----\n",
    "samples = track.reshape(-1, 18).astype(np.float32)\n",
    "y = torch.from_numpy(samples[:, 11].astype(np.int64))  # coarse goal labels\n",
    "X = torch.from_numpy(samples)\n",
    "\n",
    "# Split features across clients\n",
    "feature_dims = [9, 9]  # example: client 1 sees first 9 features, client 2 sees last 9\n",
    "client_splits = torch.split(X, feature_dims, dim=1)\n",
    "dataset = TensorDataset(*client_splits, y)\n",
    "loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "import glob\n",
    "\n",
    "class StarCraftDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, tracks_dir):\n",
    "        self.files = glob.glob(os.path.join(tracks_dir, \"*.npz\"))\n",
    "        # define feature groups (state features indices 0-10)\n",
    "        self.client_idx_groups = [\n",
    "            [0],    # unit_type\n",
    "            [1],    # unit_owner\n",
    "            [2,3,4],# pos.x, pos.y, pos.z\n",
    "            [5],    # status\n",
    "            [6],    # facing\n",
    "            [7],    # health\n",
    "            [8],    # shield\n",
    "            [9],    # energy\n",
    "            [10],   # build_progress\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        if (get_worker_info() is None):\n",
    "            files = self.files\n",
    "        else:\n",
    "            per_worker = int(np.ceil(len(self.files) / float(worker_info.num_workers)))\n",
    "            start = worker_info.id * per_worker\n",
    "            end = min(start + per_worker, len(self.files))\n",
    "            files = self.files[start:end]\n",
    "        \n",
    "        for file in files:\n",
    "            sequences = []\n",
    "            shard = np.load(file, allow_pickle = True)\n",
    "            track = shard[\"track\"]\n",
    "            track = torch.from_numpy(track[:, : , :11].astype(np.float32))  # keep only first 11 features\n",
    "            for n in range(track.shape[1]):\n",
    "                sample = track[:, n, :].astype(np.float32)\n",
    "                for i in range(track.shape[0] - self.seq_len):\n",
    "                    unit = sample[i:i + self.seq_len]\n",
    "                    label = unit[-1, 2:5]\n",
    "                    client_feats = [unit[:, idx] for idx in self.client_idx_groups]\n",
    "                    sequences.append((client_feats, label))\n",
    "\n",
    "            for seq in sequences:\n",
    "                yield seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKS_DIR = \"../StarCraft-Motion/tracks\"\n",
    "dataset = StarCraftDataset(TRACKS_DIR, seq_len=8)\n",
    "loader = DataLoader(dataset,\n",
    "                    batch_size=1024,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Transformer-based models -----\n",
    "# You can use PyTorch's built-in Transformer modules:\n",
    "#   nn.TransformerEncoderLayer & nn.TransformerEncoder\n",
    "\n",
    "def build_transformer(input_dim: int, embed_dim: int, n_heads: int, n_layers: int):\n",
    "    \"\"\"\n",
    "    Creates a Transformer encoder stack for tabular features.\n",
    "    We project features into token embeddings, then apply Transformer layers.\n",
    "    \"\"\"\n",
    "    # Project input features to embedding dimension\n",
    "    token_proj = nn.Linear(input_dim, embed_dim)\n",
    "    # Single Transformer layer configuration\n",
    "    encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=embed_dim,\n",
    "        nhead=n_heads,\n",
    "        dim_feedforward=embed_dim * 4,\n",
    "        dropout=0.1,\n",
    "        activation='relu'\n",
    "    )\n",
    "    transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    # Wrap into a single module\n",
    "    class TransformerNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.token_proj = token_proj\n",
    "            self.transformer = transformer\n",
    "\n",
    "        def forward(self, x):\n",
    "            # x: (batch_size, input_dim)\n",
    "            # treat each sample as sequence length=1 for simplicity;\n",
    "            emb = self.token_proj(x).unsqueeze(0)  # (1, batch, embed_dim)\n",
    "            out = self.transformer(emb)            # (1, batch, embed_dim)\n",
    "            return out.squeeze(0)\n",
    "\n",
    "    return TransformerNet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Client model using transformer\n",
    "class ClientModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_heads=2, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder = build_transformer(input_dim, embed_dim, n_heads, n_layers)\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Server fusion model also as transformer\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, n_clients, embed_dim, n_heads=2, n_layers=1, n_classes=10):\n",
    "        super().__init__()\n",
    "        # fusion transformer takes concatenated client embeddings as tokens\n",
    "        # self.token_proj = nn.Linear(n_clients * embed_dim, embed_dim)\n",
    "        # so I will be concatenating them all together. They come in the form \n",
    "        # each client is (batch_size, seq_len, embed_dim) so I will concatenate them along the second dimension\n",
    "        # then we'll get (bach_size, seq_len, n_clients * embed_dim)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=embed_dim,\n",
    "                                           nhead=n_heads,\n",
    "                                           dim_feedforward=embed_dim * 4,\n",
    "                                           dropout=0.1,\n",
    "                                           activation='relu')\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.classifier = nn.Linear(embed_dim, n_classes)\n",
    "    def forward(self, embeddings):\n",
    "        # embeddings: (batch, n_clients * embed_dim)\n",
    "        fused = self.token_proj(embeddings).unsqueeze(0)  # (1, batch, embed_dim)\n",
    "        enc = self.transformer(fused).squeeze(0)         # (batch, embed_dim)\n",
    "        return self.classifier(enc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Instantiate and train -----\n",
    "embed_dim = 4\n",
    "n_clients = len(feature_dims)\n",
    "n_classes = int(y.max().item() + 1)\n",
    "clients = [ClientModel(d, embed_dim) for d in feature_dims]\n",
    "fusion = FusionModel(n_clients, embed_dim, n_heads=2, n_layers=2, n_classes=n_classes)\n",
    "client_opts = [optim.Adam(c.parameters(), lr=1e-3) for c in clients]\n",
    "server_opt = optim.Adam(fusion.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "executor = ThreadPoolExecutor(max_workers=n_clients)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0.0\n",
    "    for *client_feats, labels in loader:\n",
    "        for opt in client_opts: opt.zero_grad()\n",
    "        server_opt.zero_grad()\n",
    "        # compute client embeddings in parallel\n",
    "        futures = [executor.submit(c, feat) for c, feat in zip(clients, client_feats)]\n",
    "        embeds = [f.result() for f in futures]\n",
    "        # concatenate and classify\n",
    "        concat = torch.cat(embeds, dim=1)\n",
    "        logits = fusion(concat)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        server_opt.step()\n",
    "        for opt in client_opts: opt.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: avg loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "print(\"Training complete on shard:\", shard_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
